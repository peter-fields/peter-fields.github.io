{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Diagnostics: (KL, Var) Phase Space for IOI Circuit\n",
    "\n",
    "Computes two diagnostics for each attention head in GPT-2-small:\n",
    "- **Normalized KL**: $\\hat\\rho_{\\text{eff}} / \\log n = (\\log n - H(\\hat\\pi)) / \\log n$\n",
    "- **Normalized Var**: $\\text{Var}_{\\hat\\pi}(\\log \\hat\\pi) / (\\log n)^2$\n",
    "\n",
    "Tests against the IOI circuit from Wang et al. 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "print(f\"Loaded: {model.cfg.n_layers} layers, {model.cfg.n_heads} heads/layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOI Circuit Head Labels\n",
    "From Wang et al. 2022 (arXiv:2211.00593)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IOI circuit head labels: (layer, head)\n",
    "IOI_HEADS = {\n",
    "    \"Name Mover\": [(9, 6), (9, 9), (10, 0)],\n",
    "    \"Backup Name Mover\": [(9, 0), (9, 7), (10, 1), (10, 2), (10, 6), (10, 10), (11, 2), (11, 9)],\n",
    "    \"Negative Name Mover\": [(10, 7), (11, 10)],\n",
    "    \"S-Inhibition\": [(7, 3), (7, 9), (8, 6), (8, 10)],\n",
    "    \"Induction\": [(5, 5), (6, 9)],\n",
    "    \"Duplicate Token\": [(0, 1), (3, 0)],\n",
    "    \"Previous Token\": [(2, 2), (4, 11)],\n",
    "}\n",
    "\n",
    "# Flat set for quick lookup\n",
    "ALL_CIRCUIT_HEADS = set()\n",
    "for heads in IOI_HEADS.values():\n",
    "    ALL_CIRCUIT_HEADS.update(heads)\n",
    "\n",
    "def head_role(layer, head):\n",
    "    \"\"\"Return circuit role string, or 'Non-circuit' if not in IOI circuit.\"\"\"\n",
    "    for role, heads in IOI_HEADS.items():\n",
    "        if (layer, head) in heads:\n",
    "            return role\n",
    "    return \"Non-circuit\"\n",
    "\n",
    "print(f\"Total circuit heads: {len(ALL_CIRCUIT_HEADS)}\")\n",
    "for role, heads in IOI_HEADS.items():\n",
    "    print(f\"  {role}: {heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOI Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import random\n\n# IOI templates (ABBA and BABA patterns) — one name repeats as subject\nTEMPLATES_ABBA = [\n    \"When {A} and {B} went to the store, {B} gave a drink to\",\n    \"When {A} and {B} went to the park, {B} handed a ball to\",\n    \"When {A} and {B} arrived at the office, {B} passed a note to\",\n    \"When {A} and {B} got to the restaurant, {B} offered a menu to\",\n    \"When {A} and {B} walked into the room, {B} showed a book to\",\n    \"After {A} and {B} met at the cafe, {B} sent a message to\",\n    \"After {A} and {B} sat down for dinner, {B} gave a gift to\",\n]\n\nTEMPLATES_BABA = [\n    \"When {B} and {A} went to the store, {B} gave a drink to\",\n    \"When {B} and {A} went to the park, {B} handed a ball to\",\n    \"When {B} and {A} arrived at the office, {B} passed a note to\",\n    \"When {B} and {A} got to the restaurant, {B} offered a menu to\",\n    \"When {B} and {A} walked into the room, {B} showed a book to\",\n    \"After {B} and {A} met at the cafe, {B} sent a message to\",\n    \"After {B} and {A} sat down for dinner, {B} gave a gift to\",\n]\n\n# Non-IOI controls: same structure, same length, but the subject of the second\n# clause is a THIRD name C (not A or B). No name repeats → IOI circuit shouldn't fire.\nTEMPLATES_NON_IOI = [\n    \"When {A} and {B} went to the store, {C} gave a drink to\",\n    \"When {A} and {B} went to the park, {C} handed a ball to\",\n    \"When {A} and {B} arrived at the office, {C} passed a note to\",\n    \"When {A} and {B} got to the restaurant, {C} offered a menu to\",\n    \"When {A} and {B} walked into the room, {C} showed a book to\",\n    \"After {A} and {B} met at the cafe, {C} sent a message to\",\n    \"After {A} and {B} sat down for dinner, {C} gave a gift to\",\n]\n\nNAMES = [\n    \"Mary\", \"John\", \"Alice\", \"Bob\", \"Sarah\", \"Tom\",\n    \"Emma\", \"James\", \"Lisa\", \"David\", \"Kate\", \"Mark\",\n]\n\n\ndef generate_unique_prompts_ioi(n, seed=42):\n    \"\"\"Generate n unique IOI prompts (one name repeats).\"\"\"\n    random.seed(seed)\n    templates = TEMPLATES_ABBA + TEMPLATES_BABA\n    seen = set()\n    prompts = []\n    while len(prompts) < n:\n        template = random.choice(templates)\n        a, b = random.sample(NAMES, 2)\n        prompt = template.format(A=a, B=b)\n        if prompt not in seen:\n            seen.add(prompt)\n            prompts.append(prompt)\n    return prompts\n\n\ndef generate_unique_prompts_non_ioi(n, seed=43):\n    \"\"\"Generate n unique non-IOI prompts (third name C, no repetition).\"\"\"\n    random.seed(seed)\n    seen = set()\n    prompts = []\n    while len(prompts) < n:\n        template = random.choice(TEMPLATES_NON_IOI)\n        a, b, c = random.sample(NAMES, 3)\n        prompt = template.format(A=a, B=b, C=c)\n        if prompt not in seen:\n            seen.add(prompt)\n            prompts.append(prompt)\n    return prompts\n\n\nioi_prompts = generate_unique_prompts_ioi(50)\nnon_ioi_prompts = generate_unique_prompts_non_ioi(50)\n\nassert len(set(ioi_prompts)) == 50, \"IOI prompts have duplicates!\"\nassert len(set(non_ioi_prompts)) == 50, \"Non-IOI prompts have duplicates!\"\n\nprint(f\"IOI prompts: {len(ioi_prompts)} (all unique)\")\nfor i in range(3):\n    print(f\"  {ioi_prompts[i]}\")\nprint(f\"\\nNon-IOI prompts: {len(non_ioi_prompts)} (all unique)\")\nfor i in range(3):\n    print(f\"  {non_ioi_prompts[i]}\")\n\n# Verify lengths match\nioi_lens = [model.to_tokens(p).shape[1] for p in ioi_prompts]\nnon_ioi_lens = [model.to_tokens(p).shape[1] for p in non_ioi_prompts]\nprint(f\"\\nToken lengths — IOI: {min(ioi_lens)}-{max(ioi_lens)} (mean {np.mean(ioi_lens):.1f}), \"\n      f\"Non-IOI: {min(non_ioi_lens)}-{max(non_ioi_lens)} (mean {np.mean(non_ioi_lens):.1f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diagnostics(model, prompts):\n",
    "    \"\"\"\n",
    "    For each prompt, compute normalized KL and normalized Var for all heads\n",
    "    at the final token position.\n",
    "    \n",
    "    Returns:\n",
    "        kl: (n_prompts, n_layers, n_heads)\n",
    "        var: (n_prompts, n_layers, n_heads)\n",
    "    \"\"\"\n",
    "    n_layers = model.cfg.n_layers\n",
    "    n_heads = model.cfg.n_heads\n",
    "    n_prompts = len(prompts)\n",
    "    \n",
    "    kl_all = np.zeros((n_prompts, n_layers, n_heads))\n",
    "    var_all = np.zeros((n_prompts, n_layers, n_heads))\n",
    "    \n",
    "    for p_idx, prompt in enumerate(prompts):\n",
    "        tokens = model.to_tokens(prompt)  # (1, seq_len)\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        seq_len = tokens.shape[1]\n",
    "        log_n = np.log(seq_len)\n",
    "        \n",
    "        for layer in range(n_layers):\n",
    "            # attention pattern: (1, n_heads, seq_len, seq_len)\n",
    "            attn = cache[\"pattern\", layer]  # (1, n_heads, dest, src)\n",
    "            # Take final token's attention over all source positions\n",
    "            pi = attn[0, :, -1, :]  # (n_heads, seq_len)\n",
    "            \n",
    "            log_pi = torch.log(pi + 1e-12)\n",
    "            \n",
    "            # H(pi) = -sum pi * log pi\n",
    "            entropy = -(pi * log_pi).sum(dim=-1)  # (n_heads,)\n",
    "            \n",
    "            # KL(pi || u) = log n - H(pi), normalized by log n\n",
    "            kl_norm = (log_n - entropy.cpu().numpy()) / log_n\n",
    "            \n",
    "            # Var_pi(log pi), normalized by (log n)^2\n",
    "            mean_log_pi = (pi * log_pi).sum(dim=-1, keepdim=True)  # (n_heads, 1)\n",
    "            var_log_pi = (pi * (log_pi - mean_log_pi) ** 2).sum(dim=-1)  # (n_heads,)\n",
    "            var_norm = var_log_pi.cpu().numpy() / (log_n ** 2)\n",
    "            \n",
    "            kl_all[p_idx, layer, :] = kl_norm\n",
    "            var_all[p_idx, layer, :] = var_norm\n",
    "    \n",
    "    return kl_all, var_all\n",
    "\n",
    "\n",
    "print(\"Computing diagnostics on IOI prompts...\")\n",
    "kl_ioi, var_ioi = compute_diagnostics(model, ioi_prompts)\n",
    "print(f\"  Shape: {kl_ioi.shape}\")\n",
    "\n",
    "print(\"Computing diagnostics on non-IOI prompts...\")\n",
    "kl_non_ioi, var_non_ioi = compute_diagnostics(model, non_ioi_prompts)\n",
    "print(f\"  Shape: {kl_non_ioi.shape}\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1: One Circuit Head vs One Non-Circuit Head (IOI prompts)\n",
    "\n",
    "Name Mover (9, 9) vs a non-circuit head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick heads\n",
    "circuit_head = (9, 9)   # Name Mover\n",
    "non_circuit_head = (1, 1)  # Not in any IOI circuit component\n",
    "\n",
    "# Verify\n",
    "assert circuit_head in ALL_CIRCUIT_HEADS\n",
    "assert non_circuit_head not in ALL_CIRCUIT_HEADS\n",
    "\n",
    "# Extract per-prompt values\n",
    "kl_circuit = kl_ioi[:, circuit_head[0], circuit_head[1]]\n",
    "var_circuit = var_ioi[:, circuit_head[0], circuit_head[1]]\n",
    "kl_noncircuit = kl_ioi[:, non_circuit_head[0], non_circuit_head[1]]\n",
    "var_noncircuit = var_ioi[:, non_circuit_head[0], non_circuit_head[1]]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "ax.scatter(kl_circuit, var_circuit, alpha=0.4, s=30, c=\"#d62728\",\n",
    "           label=f\"L{circuit_head[0]}H{circuit_head[1]} (Name Mover)\", zorder=2)\n",
    "ax.scatter(kl_noncircuit, var_noncircuit, alpha=0.4, s=30, c=\"#1f77b4\",\n",
    "           label=f\"L{non_circuit_head[0]}H{non_circuit_head[1]} (Non-circuit)\", zorder=2)\n",
    "\n",
    "# Averages as larger markers\n",
    "ax.scatter(kl_circuit.mean(), var_circuit.mean(), s=200, c=\"#d62728\",\n",
    "           marker=\"*\", edgecolors=\"black\", linewidths=0.8, zorder=3)\n",
    "ax.scatter(kl_noncircuit.mean(), var_noncircuit.mean(), s=200, c=\"#1f77b4\",\n",
    "           marker=\"*\", edgecolors=\"black\", linewidths=0.8, zorder=3)\n",
    "\n",
    "ax.set_xlabel(r\"Normalized KL: $\\hat{\\rho}_{\\mathrm{eff}} \\,/\\, \\log n$\", fontsize=12)\n",
    "ax.set_ylabel(r\"Normalized Var: $\\mathrm{Var}_{\\hat{\\pi}}(\\log \\hat{\\pi}) \\,/\\, (\\log n)^2$\", fontsize=12)\n",
    "ax.set_title(\"Figure 1: Circuit vs Non-Circuit Head on IOI Prompts\", fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xlim(left=-0.02)\n",
    "ax.set_ylim(bottom=-0.005)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fig1_circuit_vs_noncircuit.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2: Circuit Head — Activating vs Non-Activating Prompts\n",
    "\n",
    "Same Name Mover (9, 9): IOI prompts vs non-IOI prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, h = circuit_head\n",
    "\n",
    "kl_active = kl_ioi[:, l, h]\n",
    "var_active = var_ioi[:, l, h]\n",
    "kl_inactive = kl_non_ioi[:, l, h]\n",
    "var_inactive = var_non_ioi[:, l, h]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "ax.scatter(kl_active, var_active, alpha=0.4, s=30, c=\"#d62728\",\n",
    "           label=\"IOI prompts (circuit active)\", zorder=2)\n",
    "ax.scatter(kl_inactive, var_inactive, alpha=0.4, s=30, c=\"#7f7f7f\",\n",
    "           label=\"Non-IOI prompts (circuit inactive)\", zorder=2)\n",
    "\n",
    "# Averages\n",
    "ax.scatter(kl_active.mean(), var_active.mean(), s=200, c=\"#d62728\",\n",
    "           marker=\"*\", edgecolors=\"black\", linewidths=0.8, zorder=3)\n",
    "ax.scatter(kl_inactive.mean(), var_inactive.mean(), s=200, c=\"#7f7f7f\",\n",
    "           marker=\"*\", edgecolors=\"black\", linewidths=0.8, zorder=3)\n",
    "\n",
    "ax.set_xlabel(r\"Normalized KL: $\\hat{\\rho}_{\\mathrm{eff}} \\,/\\, \\log n$\", fontsize=12)\n",
    "ax.set_ylabel(r\"Normalized Var: $\\mathrm{Var}_{\\hat{\\pi}}(\\log \\hat{\\pi}) \\,/\\, (\\log n)^2$\", fontsize=12)\n",
    "ax.set_title(f\"Figure 2: L{l}H{h} (Name Mover) — Active vs Inactive\", fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_xlim(left=-0.02)\n",
    "ax.set_ylim(bottom=-0.005)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fig2_active_vs_inactive.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Figure 2b: Active vs Inactive — Grid Across Many Heads\n\nSame comparison as Fig 2, but for a selection of circuit heads (various roles) and non-circuit heads.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Heads to compare: one per circuit role + a few non-circuit heads\nHEADS_TO_PLOT = [\n    # Circuit heads (one per role)\n    ((9, 9),  \"Name Mover\"),\n    ((9, 6),  \"Name Mover\"),\n    ((7, 3),  \"S-Inhibition\"),\n    ((5, 5),  \"Induction\"),\n    ((3, 0),  \"Duplicate Token\"),\n    ((4, 11), \"Previous Token\"),\n    # Non-circuit heads (spread across layers)\n    ((1, 1),  \"Non-circuit\"),\n    ((3, 5),  \"Non-circuit\"),\n    ((6, 0),  \"Non-circuit\"),\n    ((11, 0), \"Non-circuit\"),\n]\n\nn_heads_plot = len(HEADS_TO_PLOT)\nncols = 5\nnrows = 2\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(4.5 * ncols, 4 * nrows),\n                          sharex=True, sharey=True)\naxes = axes.flatten()\n\nfor idx, ((l, h), role) in enumerate(HEADS_TO_PLOT):\n    ax = axes[idx]\n\n    kl_active = kl_ioi[:, l, h]\n    var_active = var_ioi[:, l, h]\n    kl_inactive = kl_non_ioi[:, l, h]\n    var_inactive = var_non_ioi[:, l, h]\n\n    ax.scatter(kl_active, var_active, alpha=0.35, s=20, c=\"#d62728\",\n               label=\"IOI\", zorder=2)\n    ax.scatter(kl_inactive, var_inactive, alpha=0.35, s=20, c=\"#7f7f7f\",\n               label=\"Non-IOI\", zorder=2)\n\n    # Averages with arrows from inactive mean to active mean\n    mean_active = (kl_active.mean(), var_active.mean())\n    mean_inactive = (kl_inactive.mean(), var_inactive.mean())\n\n    ax.scatter(*mean_active, s=150, c=\"#d62728\", marker=\"*\",\n               edgecolors=\"black\", linewidths=0.6, zorder=4)\n    ax.scatter(*mean_inactive, s=150, c=\"#7f7f7f\", marker=\"*\",\n               edgecolors=\"black\", linewidths=0.6, zorder=4)\n\n    # Arrow from inactive mean to active mean\n    ax.annotate(\"\", xy=mean_active, xytext=mean_inactive,\n                arrowprops=dict(arrowstyle=\"->\", color=\"black\",\n                                lw=1.5, connectionstyle=\"arc3,rad=0.1\"),\n                zorder=3)\n\n    in_circuit = \"IOI\" if (l, h) in ALL_CIRCUIT_HEADS else \"non\"\n    ax.set_title(f\"L{l}H{h} ({role})\", fontsize=10, fontweight=\"bold\")\n    ax.grid(True, alpha=0.2)\n\n    if idx == 0:\n        ax.legend(fontsize=7, loc=\"upper left\")\n\n# Shared axis labels\nfor ax in axes[ncols:]:\n    ax.set_xlabel(r\"Norm. KL\", fontsize=9)\nfor ax in axes[::ncols]:\n    ax.set_ylabel(r\"Norm. Var\", fontsize=9)\n\nfig.suptitle(\"Figure 2b: Active (IOI) vs Inactive (Non-IOI) — Per Head\\n\"\n             \"Arrow: inactive mean → active mean\",\n             fontsize=13, y=1.02)\nplt.tight_layout()\nplt.savefig(\"fig2b_grid_active_inactive.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Figure 2c: All S-Inhibition Heads — Does Var consistently drop on activation?\n\nL7H3 was the only circuit head in 2b where Var went *down* on activation. Check whether this is an S-Inhibition trait or specific to that head.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "S_INHIBITION_HEADS = [(7, 3), (7, 9), (8, 6), (8, 10)]\n\nfig, axes = plt.subplots(1, 4, figsize=(5 * 4, 4.5), sharex=True, sharey=True)\n\nfor idx, (l, h) in enumerate(S_INHIBITION_HEADS):\n    ax = axes[idx]\n\n    kl_active = kl_ioi[:, l, h]\n    var_active = var_ioi[:, l, h]\n    kl_inactive = kl_non_ioi[:, l, h]\n    var_inactive = var_non_ioi[:, l, h]\n\n    ax.scatter(kl_active, var_active, alpha=0.35, s=20, c=\"#d62728\",\n               label=\"IOI\", zorder=2)\n    ax.scatter(kl_inactive, var_inactive, alpha=0.35, s=20, c=\"#7f7f7f\",\n               label=\"Non-IOI\", zorder=2)\n\n    mean_active = (kl_active.mean(), var_active.mean())\n    mean_inactive = (kl_inactive.mean(), var_inactive.mean())\n\n    ax.scatter(*mean_active, s=150, c=\"#d62728\", marker=\"*\",\n               edgecolors=\"black\", linewidths=0.6, zorder=4)\n    ax.scatter(*mean_inactive, s=150, c=\"#7f7f7f\", marker=\"*\",\n               edgecolors=\"black\", linewidths=0.6, zorder=4)\n\n    ax.annotate(\"\", xy=mean_active, xytext=mean_inactive,\n                arrowprops=dict(arrowstyle=\"->\", color=\"black\",\n                                lw=1.5, connectionstyle=\"arc3,rad=0.1\"),\n                zorder=3)\n\n    # Print delta for quick reading\n    dkl = mean_active[0] - mean_inactive[0]\n    dvar = mean_active[1] - mean_inactive[1]\n    ax.set_title(f\"L{l}H{h} (S-Inhib)\\n\"\n                 f\"$\\\\Delta$KL={dkl:+.3f}, $\\\\Delta$Var={dvar:+.3f}\",\n                 fontsize=10, fontweight=\"bold\")\n    ax.grid(True, alpha=0.2)\n\n    if idx == 0:\n        ax.legend(fontsize=8, loc=\"upper left\")\n        ax.set_ylabel(r\"Norm. Var\", fontsize=10)\n\naxes[0].set_xlabel(r\"Norm. KL\", fontsize=10)\naxes[1].set_xlabel(r\"Norm. KL\", fontsize=10)\naxes[2].set_xlabel(r\"Norm. KL\", fontsize=10)\naxes[3].set_xlabel(r\"Norm. KL\", fontsize=10)\n\nfig.suptitle(\"Figure 2c: All S-Inhibition Heads — Active vs Inactive\",\n             fontsize=13, y=1.02)\nplt.tight_layout()\nplt.savefig(\"fig2c_s_inhibition.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === Figure 2d: ALL circuit heads — active vs inactive grid ===\n# Organized by role, shared axis bounds so fingerprints are directly comparable\n\n# Build list: all circuit heads grouped by role\nall_circuit_by_role = []\nfor role, heads in IOI_HEADS.items():\n    for (l, h) in heads:\n        all_circuit_by_role.append(((l, h), role))\n\nn_total = len(all_circuit_by_role)  # 23\nncols = 6\nnrows = int(np.ceil(n_total / ncols))  # 4\n\n# Pre-compute global axis bounds across all circuit heads, both prompt types\nall_kl_vals = []\nall_chi_vals = []\nfor (l, h), _ in all_circuit_by_role:\n    all_kl_vals.extend(kl_ioi[:, l, h].tolist())\n    all_kl_vals.extend(kl_non_ioi[:, l, h].tolist())\n    all_chi_vals.extend(var_ioi[:, l, h].tolist())\n    all_chi_vals.extend(var_non_ioi[:, l, h].tolist())\n\nkl_lo = min(all_kl_vals) - 0.02\nkl_hi = max(all_kl_vals) + 0.02\nchi_lo = min(all_chi_vals) - 0.01\nchi_hi = max(all_chi_vals) + 0.01\n\nROLE_COLORS = {\n    \"Name Mover\": \"#d62728\",\n    \"Backup Name Mover\": \"#ff7f0e\",\n    \"Negative Name Mover\": \"#9467bd\",\n    \"S-Inhibition\": \"#2ca02c\",\n    \"Induction\": \"#17becf\",\n    \"Duplicate Token\": \"#bcbd22\",\n    \"Previous Token\": \"#e377c2\",\n}\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(4 * ncols, 3.5 * nrows))\naxes_flat = axes.flatten()\n\nfor idx, ((l, h), role) in enumerate(all_circuit_by_role):\n    ax = axes_flat[idx]\n\n    kl_active = kl_ioi[:, l, h]\n    var_active = var_ioi[:, l, h]\n    kl_inactive = kl_non_ioi[:, l, h]\n    var_inactive = var_non_ioi[:, l, h]\n\n    color = ROLE_COLORS[role]\n\n    ax.scatter(kl_inactive, var_inactive, alpha=0.3, s=12, c=\"#7f7f7f\", zorder=2)\n    ax.scatter(kl_active, var_active, alpha=0.3, s=12, c=color, zorder=2)\n\n    mean_active = (kl_active.mean(), var_active.mean())\n    mean_inactive = (kl_inactive.mean(), var_inactive.mean())\n\n    ax.scatter(*mean_active, s=100, c=color, marker=\"*\",\n               edgecolors=\"black\", linewidths=0.5, zorder=4)\n    ax.scatter(*mean_inactive, s=100, c=\"#7f7f7f\", marker=\"*\",\n               edgecolors=\"black\", linewidths=0.5, zorder=4)\n\n    # Arrow from inactive → active\n    ax.annotate(\"\", xy=mean_active, xytext=mean_inactive,\n                arrowprops=dict(arrowstyle=\"->\", color=\"black\",\n                                lw=1.5, connectionstyle=\"arc3,rad=0.1\"),\n                zorder=3)\n\n    dkl = mean_active[0] - mean_inactive[0]\n    dchi = mean_active[1] - mean_inactive[1]\n\n    short_role = (role.replace(\"Backup Name Mover\", \"Backup NM\")\n                      .replace(\"Negative Name Mover\", \"Neg NM\")\n                      .replace(\"Name Mover\", \"NM\")\n                      .replace(\"S-Inhibition\", \"S-Inhib\")\n                      .replace(\"Duplicate Token\", \"Dup Tok\")\n                      .replace(\"Previous Token\", \"Prev Tok\")\n                      .replace(\"Induction\", \"Induct\"))\n\n    ax.set_title(f\"L{l}H{h} ({short_role})\\n\"\n                 f\"ΔKL={dkl:+.3f}  Δχ={dchi:+.3f}\",\n                 fontsize=8, fontweight=\"bold\")\n    ax.set_xlim(kl_lo, kl_hi)\n    ax.set_ylim(chi_lo, chi_hi)\n    ax.grid(True, alpha=0.2)\n    ax.tick_params(labelsize=7)\n\n# Hide empty subplots\nfor idx in range(n_total, len(axes_flat)):\n    axes_flat[idx].set_visible(False)\n\n# Axis labels on edges only\nfor i, ax in enumerate(axes_flat[:n_total]):\n    row, col = divmod(i, ncols)\n    if row == nrows - 1 or i >= n_total - ncols:\n        ax.set_xlabel(\"KL\", fontsize=7)\n    if col == 0:\n        ax.set_ylabel(\"χ\", fontsize=7)\n\nfig.suptitle(\"Figure 2d: ALL Circuit Heads — IOI (colored) vs Non-IOI (gray)\\n\"\n             \"Arrow: non-IOI mean → IOI mean  |  All axes matched\",\n             fontsize=13, y=1.01)\nplt.tight_layout()\nplt.savefig(\"fig2d_all_circuit_heads.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# Print summary table\nprint(f\"\\n{'Head':<10} {'Role':<22} {'ΔKL':>8} {'Δχ':>8} {'KL_IOI':>8} {'KL_non':>8} {'χ_IOI':>8} {'χ_non':>8}\")\nprint(\"-\" * 85)\nfor (l, h), role in all_circuit_by_role:\n    kl_a = kl_ioi[:, l, h].mean()\n    kl_i = kl_non_ioi[:, l, h].mean()\n    chi_a = var_ioi[:, l, h].mean()\n    chi_i = var_non_ioi[:, l, h].mean()\n    dkl = kl_a - kl_i\n    dchi = chi_a - chi_i\n    print(f\"L{l}H{h:<7} {role:<22} {dkl:+.4f} {dchi:+.4f} {kl_a:.4f} {kl_i:.4f} {chi_a:.4f} {chi_i:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3: All 144 Heads on IOI Prompts, Colored by Circuit Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average over IOI prompts for each head\n",
    "kl_mean = kl_ioi.mean(axis=0)   # (n_layers, n_heads)\n",
    "var_mean = var_ioi.mean(axis=0)\n",
    "\n",
    "# Color/marker scheme\n",
    "ROLE_STYLE = {\n",
    "    \"Name Mover\":          {\"c\": \"#d62728\", \"marker\": \"o\", \"s\": 80},\n",
    "    \"Backup Name Mover\":   {\"c\": \"#ff7f0e\", \"marker\": \"s\", \"s\": 60},\n",
    "    \"Negative Name Mover\": {\"c\": \"#9467bd\", \"marker\": \"D\", \"s\": 60},\n",
    "    \"S-Inhibition\":        {\"c\": \"#2ca02c\", \"marker\": \"^\", \"s\": 70},\n",
    "    \"Induction\":           {\"c\": \"#17becf\", \"marker\": \"v\", \"s\": 70},\n",
    "    \"Duplicate Token\":     {\"c\": \"#bcbd22\", \"marker\": \"<\", \"s\": 60},\n",
    "    \"Previous Token\":      {\"c\": \"#e377c2\", \"marker\": \">\", \"s\": 60},\n",
    "    \"Non-circuit\":         {\"c\": \"#cccccc\", \"marker\": \".\", \"s\": 25},\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "\n",
    "# Plot non-circuit first (background)\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    for head in range(model.cfg.n_heads):\n",
    "        role = head_role(layer, head)\n",
    "        if role == \"Non-circuit\":\n",
    "            style = ROLE_STYLE[role]\n",
    "            ax.scatter(kl_mean[layer, head], var_mean[layer, head],\n",
    "                       c=style[\"c\"], marker=style[\"marker\"], s=style[\"s\"],\n",
    "                       alpha=0.5, zorder=1)\n",
    "\n",
    "# Plot circuit heads on top with labels\n",
    "plotted_roles = set()\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    for head in range(model.cfg.n_heads):\n",
    "        role = head_role(layer, head)\n",
    "        if role != \"Non-circuit\":\n",
    "            style = ROLE_STYLE[role]\n",
    "            label = role if role not in plotted_roles else None\n",
    "            plotted_roles.add(role)\n",
    "            ax.scatter(kl_mean[layer, head], var_mean[layer, head],\n",
    "                       c=style[\"c\"], marker=style[\"marker\"], s=style[\"s\"],\n",
    "                       edgecolors=\"black\", linewidths=0.5,\n",
    "                       alpha=0.9, zorder=2, label=label)\n",
    "            ax.annotate(f\"{layer}.{head}\",\n",
    "                        (kl_mean[layer, head], var_mean[layer, head]),\n",
    "                        fontsize=6, ha=\"left\", va=\"bottom\",\n",
    "                        xytext=(3, 3), textcoords=\"offset points\")\n",
    "\n",
    "# Add non-circuit to legend\n",
    "ax.scatter([], [], c=\"#cccccc\", marker=\".\", s=25, label=\"Non-circuit\")\n",
    "\n",
    "ax.set_xlabel(r\"Normalized KL: $\\hat{\\rho}_{\\mathrm{eff}} \\,/\\, \\log n$\", fontsize=12)\n",
    "ax.set_ylabel(r\"Normalized Var: $\\mathrm{Var}_{\\hat{\\pi}}(\\log \\hat{\\pi}) \\,/\\, (\\log n)^2$\", fontsize=12)\n",
    "ax.set_title(\"Figure 3: All Heads on IOI Prompts — Averaged over 50 Prompts\", fontsize=13)\n",
    "ax.legend(fontsize=9, loc=\"upper right\")\n",
    "ax.set_xlim(left=-0.02)\n",
    "ax.set_ylim(bottom=-0.005)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fig3_all_heads_ioi.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Exploration: Is χ redundant with KL? And does ⟨z⟩ do better?\n\nCheck (1) correlation between KL and χ across heads/prompts, and (2) whether excess expected score ⟨z⟩_π − ⟨z⟩_u provides an independent second axis.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_all_diagnostics(model, prompts):\n    \"\"\"\n    Compute KL, chi (Var), and excess expected score for all heads.\n    Returns arrays of shape (n_prompts, n_layers, n_heads).\n    \"\"\"\n    n_layers = model.cfg.n_layers\n    n_heads = model.cfg.n_heads\n    n_prompts = len(prompts)\n\n    kl_all = np.zeros((n_prompts, n_layers, n_heads))\n    chi_all = np.zeros((n_prompts, n_layers, n_heads))\n    ez_excess_all = np.zeros((n_prompts, n_layers, n_heads))\n\n    for p_idx, prompt in enumerate(prompts):\n        tokens = model.to_tokens(prompt)\n        _, cache = model.run_with_cache(tokens)\n\n        seq_len = tokens.shape[1]\n        log_n = np.log(seq_len)\n\n        for layer in range(n_layers):\n            # Attention weights: (1, n_heads, seq, seq)\n            pi = cache[\"pattern\", layer][0, :, -1, :]  # (n_heads, seq_len)\n\n            # Raw scores (pre-softmax): TransformerLens stores \"attn_scores\"\n            # shape: (1, n_heads, seq, seq)\n            z = cache[\"attn_scores\", layer][0, :, -1, :]  # (n_heads, seq_len)\n\n            log_pi = torch.log(pi + 1e-12)\n\n            # KL = log n - H(pi), normalized\n            entropy = -(pi * log_pi).sum(dim=-1)\n            kl_norm = (log_n - entropy.cpu().numpy()) / log_n\n\n            # chi = Var_pi(log pi), normalized\n            mean_log_pi = (pi * log_pi).sum(dim=-1, keepdim=True)\n            var_log_pi = (pi * (log_pi - mean_log_pi) ** 2).sum(dim=-1)\n            chi_norm = var_log_pi.cpu().numpy() / (log_n ** 2)\n\n            # Excess expected score: <z>_pi - <z>_u\n            ez_pi = (pi * z).sum(dim=-1)          # (n_heads,)\n            ez_u = z.mean(dim=-1)                  # (n_heads,)\n            ez_excess = (ez_pi - ez_u).cpu().numpy()\n\n            kl_all[p_idx, layer, :] = kl_norm\n            chi_all[p_idx, layer, :] = chi_norm\n            ez_excess_all[p_idx, layer, :] = ez_excess\n\n    return kl_all, chi_all, ez_excess_all\n\n\nprint(\"Computing full diagnostics on IOI prompts...\")\nkl_ioi_full, chi_ioi, ez_ioi = compute_all_diagnostics(model, ioi_prompts)\nprint(\"Computing full diagnostics on non-IOI prompts...\")\nkl_nonioi_full, chi_nonioi, ez_nonioi = compute_all_diagnostics(model, non_ioi_prompts)\nprint(\"Done!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === Part 1: KL vs chi correlation ===\n# Average over IOI prompts per head, then flatten\nkl_flat = kl_ioi_full.mean(axis=0).flatten()\nchi_flat = chi_ioi.mean(axis=0).flatten()\nez_flat = ez_ioi.mean(axis=0).flatten()\n\ncorr_kl_chi = np.corrcoef(kl_flat, chi_flat)[0, 1]\ncorr_kl_ez = np.corrcoef(kl_flat, ez_flat)[0, 1]\ncorr_chi_ez = np.corrcoef(chi_flat, ez_flat)[0, 1]\n\nprint(\"=== Correlations (averaged over IOI prompts, all 144 heads) ===\")\nprint(f\"  corr(KL, χ)      = {corr_kl_chi:.3f}\")\nprint(f\"  corr(KL, ⟨z⟩_ex) = {corr_kl_ez:.3f}\")\nprint(f\"  corr(χ,  ⟨z⟩_ex) = {corr_chi_ez:.3f}\")\n\n# Also check per-prompt (not averaged): flatten everything\nkl_all_flat = kl_ioi_full.flatten()\nchi_all_flat = chi_ioi.flatten()\nez_all_flat = ez_ioi.flatten()\n\ncorr_kl_chi_pp = np.corrcoef(kl_all_flat, chi_all_flat)[0, 1]\ncorr_kl_ez_pp = np.corrcoef(kl_all_flat, ez_all_flat)[0, 1]\n\nprint(f\"\\n=== Per-prompt correlations (all 144 heads × 50 prompts = 7200 points) ===\")\nprint(f\"  corr(KL, χ)      = {corr_kl_chi_pp:.3f}\")\nprint(f\"  corr(KL, ⟨z⟩_ex) = {corr_kl_ez_pp:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# === Is Δχ redundant with ΔKL? ===\n# Compute deltas\ndelta_kl = kl_ioi_full.mean(axis=0) - kl_nonioi_full.mean(axis=0)  # (12, 12)\ndelta_chi = chi_ioi.mean(axis=0) - chi_nonioi.mean(axis=0)\n\nkl_ioi_mean = kl_ioi_full.mean(axis=0)\nkl_nonioi_mean = kl_nonioi_full.mean(axis=0)\nchi_ioi_mean = chi_ioi.mean(axis=0)\nchi_nonioi_mean = chi_nonioi.mean(axis=0)\n\n# Flatten for correlations\ndkl_flat = delta_kl.flatten()\ndchi_flat = delta_chi.flatten()\n\n# 1) All 144 heads\nr_all = np.corrcoef(dkl_flat, dchi_flat)[0, 1]\n\n# 2) Circuit heads only\ndkl_circuit = np.array([delta_kl[l, h] for l, h in ALL_CIRCUIT_HEADS])\ndchi_circuit = np.array([delta_chi[l, h] for l, h in ALL_CIRCUIT_HEADS])\nr_circuit = np.corrcoef(dkl_circuit, dchi_circuit)[0, 1]\n\n# 3) Non-circuit heads only\ndkl_noncircuit = np.array([delta_kl[l, h] for l in range(12) for h in range(12)\n                            if (l, h) not in ALL_CIRCUIT_HEADS])\ndchi_noncircuit = np.array([delta_chi[l, h] for l in range(12) for h in range(12)\n                             if (l, h) not in ALL_CIRCUIT_HEADS])\nr_noncircuit = np.corrcoef(dkl_noncircuit, dchi_noncircuit)[0, 1]\n\n# 4) KL levels (not deltas) on IOI prompts — for comparison\nkl_ioi_flat = kl_ioi_mean.flatten()\nchi_ioi_flat = chi_ioi_mean.flatten()\nr_levels = np.corrcoef(kl_ioi_flat, chi_ioi_flat)[0, 1]\n\n# 5) |ΔKL| vs |Δχ| (absolute magnitudes)\nr_abs = np.corrcoef(np.abs(dkl_flat), np.abs(dchi_flat))[0, 1]\nr_abs_circuit = np.corrcoef(np.abs(dkl_circuit), np.abs(dchi_circuit))[0, 1]\n\nprint(\"=== Is Δχ redundant with ΔKL? ===\\n\")\nprint(\"Correlations:\")\nprint(f\"  corr(ΔKL, Δχ) — all 144 heads:    r = {r_all:.3f}\")\nprint(f\"  corr(ΔKL, Δχ) — circuit (n=23):    r = {r_circuit:.3f}\")\nprint(f\"  corr(ΔKL, Δχ) — non-circuit (n=121): r = {r_noncircuit:.3f}\")\nprint(f\"  corr(|ΔKL|, |Δχ|) — all 144:       r = {r_abs:.3f}\")\nprint(f\"  corr(|ΔKL|, |Δχ|) — circuit (n=23): r = {r_abs_circuit:.3f}\")\nprint(f\"\\nFor comparison (levels, not deltas):\")\nprint(f\"  corr(KL, χ) on IOI prompts:         r = {r_levels:.3f}\")\n\n# Scatter: ΔKL vs Δχ\nfig, axes = plt.subplots(1, 2, figsize=(14, 5.5))\n\n# Panel 1: all heads\nax = axes[0]\nfor layer in range(12):\n    for head in range(12):\n        role = head_role(layer, head)\n        c = ROLE_STYLE.get(role, ROLE_STYLE[\"Non-circuit\"])[\"c\"]\n        alpha = 0.9 if role != \"Non-circuit\" else 0.4\n        s = 50 if role != \"Non-circuit\" else 15\n        ax.scatter(delta_kl[layer, head], delta_chi[layer, head],\n                   c=c, s=s, alpha=alpha, edgecolors=\"black\" if role != \"Non-circuit\" else \"none\",\n                   linewidths=0.4)\n        if role != \"Non-circuit\":\n            ax.annotate(f\"{layer}.{head}\", (delta_kl[layer, head], delta_chi[layer, head]),\n                        fontsize=5, xytext=(3, 3), textcoords=\"offset points\")\n\nax.axhline(0, color=\"black\", lw=0.5, ls=\"--\")\nax.axvline(0, color=\"black\", lw=0.5, ls=\"--\")\nax.set_xlabel(\"ΔKL (IOI − non-IOI)\", fontsize=11)\nax.set_ylabel(\"Δχ (IOI − non-IOI)\", fontsize=11)\nax.set_title(f\"ΔKL vs Δχ — all 144 heads (r = {r_all:.3f})\", fontsize=12)\nax.grid(True, alpha=0.2)\n\n# Panel 2: circuit heads only, labeled by role\nax = axes[1]\nplotted_roles = set()\nfor role, heads in IOI_HEADS.items():\n    for (l, h) in heads:\n        label = role if role not in plotted_roles else None\n        plotted_roles.add(role)\n        ax.scatter(delta_kl[l, h], delta_chi[l, h],\n                   c=ROLE_STYLE[role][\"c\"], s=80, alpha=0.9,\n                   marker=ROLE_STYLE[role][\"marker\"],\n                   edgecolors=\"black\", linewidths=0.5, label=label)\n        ax.annotate(f\"L{l}H{h}\", (delta_kl[l, h], delta_chi[l, h]),\n                    fontsize=7, xytext=(4, 4), textcoords=\"offset points\")\n\nax.axhline(0, color=\"black\", lw=0.5, ls=\"--\")\nax.axvline(0, color=\"black\", lw=0.5, ls=\"--\")\nax.set_xlabel(\"ΔKL (IOI − non-IOI)\", fontsize=11)\nax.set_ylabel(\"Δχ (IOI − non-IOI)\", fontsize=11)\nax.set_title(f\"ΔKL vs Δχ — circuit heads only (r = {r_circuit:.3f})\", fontsize=12)\nax.legend(fontsize=7, loc=\"best\")\nax.grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.savefig(\"delta_kl_vs_delta_chi.png\", dpi=300, bbox_inches=\"tight\")\nplt.show()\n\n# Print decoupled heads (|ΔKL| < 0.05 but |Δχ| > 0.03)\nprint(\"\\nDecoupled heads (|ΔKL| < 0.05, |Δχ| > 0.03):\")\nprint(f\"{'Head':<12} {'Role':<22} {'ΔKL':>8} {'Δχ':>8} {'KL_IOI':>8} {'KL_non':>8} {'χ_IOI':>8} {'χ_non':>8}\")\nprint(\"-\" * 90)\nfor layer in range(12):\n    for head in range(12):\n        dk = delta_kl[layer, head]\n        dc = delta_chi[layer, head]\n        if abs(dk) < 0.05 and abs(dc) > 0.03:\n            role = head_role(layer, head)\n            print(f\"L{layer}H{head:<7} {role:<22} {dk:+.4f} {dc:+.4f} \"\n                  f\"{kl_ioi_mean[layer,head]:.4f} {kl_nonioi_mean[layer,head]:.4f} \"\n                  f\"{chi_ioi_mean[layer,head]:.4f} {chi_nonioi_mean[layer,head]:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# For each head: compute mean KL and chi on IOI vs non-IOI, then deltas\ndelta_kl = kl_ioi_full.mean(axis=0) - kl_nonioi_full.mean(axis=0)  # (12, 12)\ndelta_chi = chi_ioi.mean(axis=0) - chi_nonioi.mean(axis=0)\n\nkl_ioi_mean = kl_ioi_full.mean(axis=0)\nkl_nonioi_mean = kl_nonioi_full.mean(axis=0)\nchi_ioi_mean = chi_ioi.mean(axis=0)\nchi_nonioi_mean = chi_nonioi.mean(axis=0)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5.5))\n\n# Panel 1: ΔKL vs Δχ for all heads\nax = axes[0]\nfor layer in range(12):\n    for head in range(12):\n        role = head_role(layer, head)\n        c = ROLE_STYLE.get(role, ROLE_STYLE[\"Non-circuit\"])[\"c\"]\n        alpha = 0.9 if role != \"Non-circuit\" else 0.4\n        s = 50 if role != \"Non-circuit\" else 15\n        ax.scatter(delta_kl[layer, head], delta_chi[layer, head],\n                   c=c, s=s, alpha=alpha, edgecolors=\"black\" if role != \"Non-circuit\" else \"none\",\n                   linewidths=0.4)\n        if role != \"Non-circuit\":\n            ax.annotate(f\"{layer}.{head}\", (delta_kl[layer, head], delta_chi[layer, head]),\n                        fontsize=5, xytext=(3, 3), textcoords=\"offset points\")\n\nax.axhline(0, color=\"black\", lw=0.5, ls=\"--\")\nax.axvline(0, color=\"black\", lw=0.5, ls=\"--\")\nax.set_xlabel(\"ΔKL (IOI − non-IOI)\", fontsize=11)\nax.set_ylabel(\"Δχ (IOI − non-IOI)\", fontsize=11)\nax.set_title(\"ΔKL vs Δχ: does χ decouple from KL?\", fontsize=12)\nax.grid(True, alpha=0.2)\n\n# Panel 2: Highlight heads where |ΔKL| < 0.05 but |Δχ| > 0.03\n# (KL barely changes, chi changes meaningfully)\nax = axes[1]\nfor layer in range(12):\n    for head in range(12):\n        dk = delta_kl[layer, head]\n        dc = delta_chi[layer, head]\n        role = head_role(layer, head)\n\n        is_decoupled = abs(dk) < 0.05 and abs(dc) > 0.03\n        if is_decoupled:\n            ax.scatter(dk, dc, c=\"red\", s=60, alpha=0.9,\n                       edgecolors=\"black\", linewidths=0.5, zorder=3)\n            ax.annotate(f\"L{layer}H{head} ({role})\",\n                        (dk, dc), fontsize=7, xytext=(5, 5),\n                        textcoords=\"offset points\")\n        else:\n            ax.scatter(dk, dc, c=\"#cccccc\", s=15, alpha=0.4, zorder=1)\n\nax.axhline(0, color=\"black\", lw=0.5, ls=\"--\")\nax.axvline(0, color=\"black\", lw=0.5, ls=\"--\")\nax.set_xlabel(\"ΔKL (IOI − non-IOI)\", fontsize=11)\nax.set_ylabel(\"Δχ (IOI − non-IOI)\", fontsize=11)\nax.set_title(\"Heads where KL barely changes but χ does\\n(|ΔKL| < 0.05, |Δχ| > 0.03)\", fontsize=11)\nax.grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.savefig(\"delta_kl_vs_delta_chi.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\n\n# Print the decoupled heads\nprint(\"\\nDecoupled heads (|ΔKL| < 0.05, |Δχ| > 0.03):\")\nprint(f\"{'Head':<12} {'Role':<22} {'ΔKL':>8} {'Δχ':>8} {'KL_IOI':>8} {'KL_non':>8} {'χ_IOI':>8} {'χ_non':>8}\")\nprint(\"-\" * 90)\nfor layer in range(12):\n    for head in range(12):\n        dk = delta_kl[layer, head]\n        dc = delta_chi[layer, head]\n        if abs(dk) < 0.05 and abs(dc) > 0.03:\n            role = head_role(layer, head)\n            print(f\"L{layer}H{head:<7} {role:<22} {dk:+.4f} {dc:+.4f} \"\n                  f\"{kl_ioi_mean[layer,head]:.4f} {kl_nonioi_mean[layer,head]:.4f} \"\n                  f\"{chi_ioi_mean[layer,head]:.4f} {chi_nonioi_mean[layer,head]:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Where do non-activated circuit heads actually attend?\n\nVerify directly: when circuit heads have high KL on non-IOI prompts, what tokens are they peaked on?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# For a selection of circuit heads, look at where they attend on non-IOI prompts\n# For each head & prompt: find the argmax token and the top-3 tokens with their weights\n\nHEADS_TO_INSPECT = [\n    ((9, 9),  \"Name Mover\"),\n    ((9, 6),  \"Name Mover\"),\n    ((7, 3),  \"S-Inhibition\"),\n    ((5, 5),  \"Induction\"),\n    ((3, 0),  \"Duplicate Token\"),\n    ((4, 11), \"Previous Token\"),\n]\n\nprint(\"=== Non-IOI prompts: where do circuit heads attend? ===\\n\")\n\nfor (l, h), role in HEADS_TO_INSPECT:\n    # Collect argmax position stats across all non-IOI prompts\n    argmax_positions = []  # relative position (0 = first token, -1 = last)\n    argmax_tokens = []\n    max_weights = []\n\n    for prompt in non_ioi_prompts[:20]:  # first 20 for readability\n        tokens = model.to_tokens(prompt)\n        _, cache = model.run_with_cache(tokens)\n        pi = cache[\"pattern\", l][0, h, -1, :]  # (seq_len,)\n        seq_len = tokens.shape[1]\n\n        # Top-3 positions\n        topk = torch.topk(pi, k=min(3, seq_len))\n        top_positions = topk.indices.cpu().numpy()\n        top_weights = topk.values.cpu().numpy()\n\n        # Decode tokens at those positions\n        token_strs = [model.to_string(tokens[0, pos:pos+1]) for pos in top_positions]\n\n        argmax_positions.append(top_positions[0])\n        argmax_tokens.append(token_strs[0])\n        max_weights.append(top_weights[0])\n\n    # Summary\n    argmax_positions = np.array(argmax_positions)\n    max_weights = np.array(max_weights)\n\n    # Count how often argmax is position 0 (BOS)\n    bos_frac = (argmax_positions == 0).mean()\n    # Count how often argmax is the last or second-to-last position\n    # (we look at attention FROM the last token, so prev = seq_len - 2)\n\n    print(f\"L{l}H{h} ({role}):\")\n    print(f\"  Argmax is BOS (pos 0):  {bos_frac:.0%}\")\n    print(f\"  Mean max weight:        {max_weights.mean():.3f} ± {max_weights.std():.3f}\")\n    print(f\"  Most common argmax tokens: \", end=\"\")\n\n    from collections import Counter\n    tok_counts = Counter(argmax_tokens)\n    for tok, cnt in tok_counts.most_common(5):\n        print(f\"'{tok.strip()}'({cnt})\", end=\"  \")\n    print(f\"\\n  Argmax positions: {argmax_positions.tolist()}\")\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Formal test: is |ΔKL| larger for circuit heads?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from scipy import stats\n\nabs_dkl_circuit = []\nabs_dkl_noncircuit = []\n\nfor layer in range(12):\n    for head in range(12):\n        val = abs(delta_kl[layer, head])\n        if (layer, head) in ALL_CIRCUIT_HEADS:\n            abs_dkl_circuit.append(val)\n        else:\n            abs_dkl_noncircuit.append(val)\n\nabs_dkl_circuit = np.array(abs_dkl_circuit)\nabs_dkl_noncircuit = np.array(abs_dkl_noncircuit)\n\n# Mann-Whitney U test (non-parametric, doesn't assume normality)\nu_stat, p_val = stats.mannwhitneyu(abs_dkl_circuit, abs_dkl_noncircuit, alternative='greater')\n\nprint(f\"=== |ΔKL| comparison: circuit vs non-circuit heads ===\")\nprint(f\"  Circuit heads (n={len(abs_dkl_circuit)}):     mean |ΔKL| = {abs_dkl_circuit.mean():.4f} ± {abs_dkl_circuit.std():.4f}\")\nprint(f\"  Non-circuit heads (n={len(abs_dkl_noncircuit)}): mean |ΔKL| = {abs_dkl_noncircuit.mean():.4f} ± {abs_dkl_noncircuit.std():.4f}\")\nprint(f\"  Mann-Whitney U (circuit > non-circuit): U = {u_stat:.0f}, p = {p_val:.4f}\")\nprint()\n\n# Also do the same for |Δχ|\nabs_dchi_circuit = np.array([abs(delta_chi[l, h]) for l, h in ALL_CIRCUIT_HEADS])\nabs_dchi_noncircuit = np.array([abs(delta_chi[l, h]) for l in range(12) for h in range(12)\n                                 if (l, h) not in ALL_CIRCUIT_HEADS])\n\nu_stat2, p_val2 = stats.mannwhitneyu(abs_dchi_circuit, abs_dchi_noncircuit, alternative='greater')\n\nprint(f\"=== |Δχ| comparison: circuit vs non-circuit heads ===\")\nprint(f\"  Circuit heads:     mean |Δχ| = {abs_dchi_circuit.mean():.4f} ± {abs_dchi_circuit.std():.4f}\")\nprint(f\"  Non-circuit heads: mean |Δχ| = {abs_dchi_noncircuit.mean():.4f} ± {abs_dchi_noncircuit.std():.4f}\")\nprint(f\"  Mann-Whitney U (circuit > non-circuit): U = {u_stat2:.0f}, p = {p_val2:.4f}\")\n\n# Histogram\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\nax = axes[0]\nax.hist(abs_dkl_noncircuit, bins=20, alpha=0.5, color=\"#cccccc\", label=\"Non-circuit\", density=True)\nax.hist(abs_dkl_circuit, bins=12, alpha=0.7, color=\"#d62728\", label=\"Circuit\", density=True)\nax.set_xlabel(\"|ΔKL| (IOI − non-IOI)\", fontsize=11)\nax.set_ylabel(\"Density\", fontsize=11)\nax.set_title(f\"|ΔKL| distribution (p = {p_val:.4f})\", fontsize=12)\nax.legend()\nax.grid(True, alpha=0.2)\n\nax = axes[1]\nax.hist(abs_dchi_noncircuit, bins=20, alpha=0.5, color=\"#cccccc\", label=\"Non-circuit\", density=True)\nax.hist(abs_dchi_circuit, bins=12, alpha=0.7, color=\"#d62728\", label=\"Circuit\", density=True)\nax.set_xlabel(\"|Δχ| (IOI − non-IOI)\", fontsize=11)\nax.set_ylabel(\"Density\", fontsize=11)\nax.set_title(f\"|Δχ| distribution (p = {p_val2:.4f})\", fontsize=12)\nax.legend()\nax.grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.savefig(\"delta_distributions.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Context length confound check\n\nDo IOI and non-IOI prompts have different token lengths? If so, normalization by log n may not fully account for it.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "ioi_lengths = [model.to_tokens(p).shape[1] for p in ioi_prompts]\nnon_ioi_lengths = [model.to_tokens(p).shape[1] for p in non_ioi_prompts]\n\nprint(f\"=== Token lengths ===\")\nprint(f\"  IOI:     mean = {np.mean(ioi_lengths):.1f}, std = {np.std(ioi_lengths):.1f}, \"\n      f\"range = [{min(ioi_lengths)}, {max(ioi_lengths)}]\")\nprint(f\"  Non-IOI: mean = {np.mean(non_ioi_lengths):.1f}, std = {np.std(non_ioi_lengths):.1f}, \"\n      f\"range = [{min(non_ioi_lengths)}, {max(non_ioi_lengths)}]\")\n\nu_len, p_len = stats.mannwhitneyu(ioi_lengths, non_ioi_lengths, alternative='two-sided')\nprint(f\"  Mann-Whitney U (two-sided): p = {p_len:.4f}\")\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 3.5))\nax.hist(ioi_lengths, bins=range(min(ioi_lengths + non_ioi_lengths),\n        max(ioi_lengths + non_ioi_lengths) + 2),\n        alpha=0.6, color=\"#d62728\", label=\"IOI\")\nax.hist(non_ioi_lengths, bins=range(min(ioi_lengths + non_ioi_lengths),\n        max(ioi_lengths + non_ioi_lengths) + 2),\n        alpha=0.6, color=\"#7f7f7f\", label=\"Non-IOI\")\nax.set_xlabel(\"Token length\", fontsize=11)\nax.set_ylabel(\"Count\", fontsize=11)\nax.set_title(\"Prompt token lengths\", fontsize=12)\nax.legend()\nax.grid(True, alpha=0.2)\nplt.tight_layout()\nplt.savefig(\"prompt_lengths.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Layer depth confound\n\nDo KL and χ vary systematically with layer depth, independent of circuit role?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# For each layer, compute mean KL and chi across all heads (non-circuit only, to avoid\n# circuit heads biasing the layer-level stats)\nkl_by_layer_ioi = []\nchi_by_layer_ioi = []\nkl_by_layer_nonioi = []\nchi_by_layer_nonioi = []\n\nfor layer in range(12):\n    non_circ_heads = [h for h in range(12) if (layer, h) not in ALL_CIRCUIT_HEADS]\n    kl_by_layer_ioi.append([kl_ioi_full[:, layer, h].mean() for h in non_circ_heads])\n    chi_by_layer_ioi.append([chi_ioi[:, layer, h].mean() for h in non_circ_heads])\n    kl_by_layer_nonioi.append([kl_nonioi_full[:, layer, h].mean() for h in non_circ_heads])\n    chi_by_layer_nonioi.append([chi_nonioi[:, layer, h].mean() for h in non_circ_heads])\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# KL by layer\nax = axes[0]\nbp1 = ax.boxplot(kl_by_layer_ioi, positions=np.arange(12) - 0.15, widths=0.25,\n                  patch_artist=True, boxprops=dict(facecolor=\"#d62728\", alpha=0.5),\n                  medianprops=dict(color=\"black\"), flierprops=dict(markersize=3))\nbp2 = ax.boxplot(kl_by_layer_nonioi, positions=np.arange(12) + 0.15, widths=0.25,\n                  patch_artist=True, boxprops=dict(facecolor=\"#7f7f7f\", alpha=0.5),\n                  medianprops=dict(color=\"black\"), flierprops=dict(markersize=3))\nax.set_xlabel(\"Layer\", fontsize=11)\nax.set_ylabel(\"Normalized KL (non-circuit heads only)\", fontsize=11)\nax.set_title(\"KL by layer depth\", fontsize=12)\nax.set_xticks(range(12))\nax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0]], [\"IOI\", \"Non-IOI\"], fontsize=9)\nax.grid(True, alpha=0.2)\n\n# Chi by layer\nax = axes[1]\nbp1 = ax.boxplot(chi_by_layer_ioi, positions=np.arange(12) - 0.15, widths=0.25,\n                  patch_artist=True, boxprops=dict(facecolor=\"#d62728\", alpha=0.5),\n                  medianprops=dict(color=\"black\"), flierprops=dict(markersize=3))\nbp2 = ax.boxplot(chi_by_layer_nonioi, positions=np.arange(12) + 0.15, widths=0.25,\n                  patch_artist=True, boxprops=dict(facecolor=\"#7f7f7f\", alpha=0.5),\n                  medianprops=dict(color=\"black\"), flierprops=dict(markersize=3))\nax.set_xlabel(\"Layer\", fontsize=11)\nax.set_ylabel(\"Normalized χ (non-circuit heads only)\", fontsize=11)\nax.set_title(\"χ by layer depth\", fontsize=12)\nax.set_xticks(range(12))\nax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0]], [\"IOI\", \"Non-IOI\"], fontsize=9)\nax.grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.savefig(\"layer_depth.png\", dpi=150, bbox_inches=\"tight\")\nplt.show()\n\n# Correlation of layer index with mean KL/chi (non-circuit only)\nlayer_indices = []\nkl_vals = []\nchi_vals = []\nfor layer in range(12):\n    for h in range(12):\n        if (layer, h) not in ALL_CIRCUIT_HEADS:\n            layer_indices.append(layer)\n            kl_vals.append(kl_ioi_full[:, layer, h].mean())\n            chi_vals.append(chi_ioi[:, layer, h].mean())\n\nr_layer_kl = np.corrcoef(layer_indices, kl_vals)[0, 1]\nr_layer_chi = np.corrcoef(layer_indices, chi_vals)[0, 1]\nprint(f\"\\nCorrelation with layer depth (non-circuit heads, IOI prompts):\")\nprint(f\"  corr(layer, KL) = {r_layer_kl:.3f}\")\nprint(f\"  corr(layer, χ)  = {r_layer_chi:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IOI prompt averages by circuit role:\")\n",
    "print(f\"{'Role':<25} {'KL (mean±std)':<20} {'Var (mean±std)'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for role in list(IOI_HEADS.keys()) + [\"Non-circuit\"]:\n",
    "    if role == \"Non-circuit\":\n",
    "        heads = [(l, h) for l in range(12) for h in range(12)\n",
    "                 if (l, h) not in ALL_CIRCUIT_HEADS]\n",
    "    else:\n",
    "        heads = IOI_HEADS[role]\n",
    "    \n",
    "    kls = [kl_ioi[:, l, h].mean() for l, h in heads]\n",
    "    vars_ = [var_ioi[:, l, h].mean() for l, h in heads]\n",
    "    \n",
    "    print(f\"{role:<25} {np.mean(kls):.4f} ± {np.std(kls):.4f}     {np.mean(vars_):.4f} ± {np.std(vars_):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}